# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2014, moco_beta
# This file is distributed under the same license as the Redis Documentation (Japanese) package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Redis Documentation (Japanese) 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2014-07-31 23:30+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/topics/distlock.rst:2
# 204da7c8faff43a1a0a0cdc6ca615169
msgid "Distributed locks with Redis"
msgstr ""

#: ../../source/topics/distlock.rst:4
# 06b38a4496254a7fa95316e21b6d8314
msgid "Distributed locks are a very useful primitive in many environments where different processes require to operate with shared resources in a mutually exclusive way."
msgstr ""

#: ../../source/topics/distlock.rst:8
# fa20df52bd9d4cc18e0a9b11381dc90c
msgid "There are a number of libraries and blog posts describing how to implement a DLM (Distributed Lock Manager) with Redis, but every library use a different approach, and many use a simple approach with lower guarantees compared to what can be achieved with slightly more complex designs."
msgstr ""

#: ../../source/topics/distlock.rst:14
# a69c74bf38354e48846ab8e70b150ac9
msgid "This page is an attempt to provide a more canonical algorithm to implement distributed locks with Redis. We propose an algorithm, called **Redlock**, which implements a DLM which we believe to be safer than the vanilla single instance approach. We hope that the community will analyze it, provide feedbacks, and use it as a starting point for the implementations or more complex or alternative designs."
msgstr ""

#: ../../source/topics/distlock.rst:22
# ac1b92aa938e4fa1b215edb8e7c2304d
msgid "Implementations"
msgstr ""

#: ../../source/topics/distlock.rst:24
# 9d3eaeada92d414483d666bf1dacc7a1
msgid "Before to describe the algorithm, here there are a few links at implementations already available, that can be used as a reference."
msgstr ""

#: ../../source/topics/distlock.rst:27
# 87d4d68ecf2442ae874ac200de71c8fe
msgid "`Redlock-rb <https://github.com/antirez/redlock-rb>`__ (Ruby implementation)."
msgstr ""

#: ../../source/topics/distlock.rst:29
# 54a9061f3ca24d9e8e67a35f69a49a78
msgid "`Redlock-php <https://github.com/ronnylt/redlock-php>`__ (PHP implementation)."
msgstr ""

#: ../../source/topics/distlock.rst:31
# b416535c59ce4d259f2616db3c976c7e
msgid "`Redsync.go <https://github.com/hjr265/redsync.go>`__ (Go implementation)."
msgstr ""

#: ../../source/topics/distlock.rst:33
# 28dadd7ff23e4c498242915272ae4a33
msgid "`Redisson <https://github.com/mrniko/redisson>`__ (Java implementation)."
msgstr ""

#: ../../source/topics/distlock.rst:37
# 2e3e4930bb154cdbbf655bd774f3db53
msgid "Safety and Liveness guarantees"
msgstr ""

#: ../../source/topics/distlock.rst:39
# 418429070c6645cf94352ddce42aadde
msgid "We are going to model our design with just three properties, that are from our point of view the minimum guarantees needed to use distributed locks in an effective way."
msgstr ""

#: ../../source/topics/distlock.rst:43
# d3ecea33945d4a529f4a9987b500cfa2
msgid "Safety property: Mutual exclusion. At any given moment, only one client can hold a lock."
msgstr ""

#: ../../source/topics/distlock.rst:45
# 4ae5ca5391d44d2ab549b2a8dc71bdc5
msgid "Liveness property A: Deadlocks free. Eventually it is always possible to acquire a lock, even if the client that locked a resource crashed or gets partitioned."
msgstr ""

#: ../../source/topics/distlock.rst:48
# 3b64008b0f2a49af8a3cbd6dbde1e3c1
msgid "Liveness property B: Fault tolerance. As long as the majority of Redis nodes are up, clients are able to acquire and release locks."
msgstr ""

#: ../../source/topics/distlock.rst:52
# 9941d8cf016b4a278393a1603266eb78
msgid "Why failover based implementations are not enough"
msgstr ""

#: ../../source/topics/distlock.rst:54
# 886750e1cd0d407c8be1f517aa2726bf
msgid "To understand what we want to improve, let’s analyze the current state of affairs with most Redis-based distributed locks libraries."
msgstr ""

#: ../../source/topics/distlock.rst:57
# 8d7960f1b18a4c49964d77aab9b5cdaf
msgid "The simple way to use Redis to lock a resource is to create a key into an instance. The key is usually created with a limited time to live, using Redis expires feature, so that eventually it gets released one way or the other (property 2 in our list). When the client needs to release the resource, it deletes the key."
msgstr ""

#: ../../source/topics/distlock.rst:63
# b4048c38634545649f4ea58be73eb36e
msgid "Superficially this works well, but there is a problem: this is a single point of failure in our architecture. What happens if the Redis master goes down? Well, let’s add a slave! And use it if the master is unavailable. This is unfortunately not viable. By doing so we can’t implement our safety property of the mutual exclusion, because Redis replication is asynchronous."
msgstr ""

#: ../../source/topics/distlock.rst:70
# 0bc6559ae17943b2b2479b0d87664a5d
msgid "This is an obvious race condition with this model:"
msgstr ""

#: ../../source/topics/distlock.rst:72
# 8f28273a800549dd95ac7b22343e82be
msgid "Client A acquires the lock into the master."
msgstr ""

#: ../../source/topics/distlock.rst:73
# 42183bd4b6ae4f2c91693c7fa0619590
msgid "The master crashes before the write to the key is transmitted to the slave."
msgstr ""

#: ../../source/topics/distlock.rst:75
# 52c808fde2b641a2a1bb47f37eb669c6
msgid "The slave gets promoted to master."
msgstr ""

#: ../../source/topics/distlock.rst:76
# 98c227fe1f4f491aa94e40e15b7c204e
msgid "Client B acquires the lock to the same resource A already holds a lock for. **SAFETY VIOLATION!**"
msgstr ""

#: ../../source/topics/distlock.rst:79
# ad47832fd7264fc282dafda8f131e9a7
msgid "Sometimes it is perfectly fine that under special circumstances, like during a failure, multiple clients can hold the lock at the same time. If this is the case, you can use your replication based solution. Otherwise we suggest to implement the solution described in this document."
msgstr ""

#: ../../source/topics/distlock.rst:86
# 194bd687b3b34d8faf9d6d9a655a0abd
msgid "Correct implementation with a single instance"
msgstr ""

#: ../../source/topics/distlock.rst:88
# f65188bdd65040a88b43424a8340f150
msgid "Before to try to overcome the limitation of the single instance setup described above, let’s check how to do it correctly in this simple case, since this is actually a viable solution in applications where a race condition from time to time is acceptable, and because locking into a single instance is the foundation we’ll use for the distributed algorithm described here."
msgstr ""

#: ../../source/topics/distlock.rst:95
# 6e4d7efd0e1240ba91d74166203623b7
msgid "To acquire the lock, the way to go is the following:"
msgstr ""

#: ../../source/topics/distlock.rst:101
# 7be4f63d022147429bb5febd088ac7e7
msgid "The command will set the key only if it does not already exist (NX option), with an expire of 30000 milliseconds (PX option). The key is set to a value “my\\_random\\_value”. This value requires to be unique across all the clients and all the locks requests."
msgstr ""

#: ../../source/topics/distlock.rst:106
# f3402069a16a48e89f9567cc065ed205
msgid "Basically the random value is used in order to release the lock in a safe way, with a script that tells Redis: remove the key only if exists and the value stored at the key is exactly the one I expect to be. This is accomplished by the following Lua script:"
msgstr ""

#: ../../source/topics/distlock.rst:119
# a0c12387a02846c09132ba0cee020152
msgid "This is important in order to avoid removing a lock that was created by another client. For example a client may acquire the lock, get blocked into some operation for longer than the lock validity time (the time at which the key will expire), and later remove the lock, that was already acquired by some other client. Using just DEL is not safe as a client may remove the lock of another client. With the above script instead every lock is “signed” with a random string, so the lock will be removed only if it is still the one that was set by the client trying to remove it."
msgstr ""

#: ../../source/topics/distlock.rst:129
# df51b6f05e1348b8bc657c4177bc8c43
msgid "What this random string should be? I assume it’s 20 bytes from /dev/urandom, but you can find cheaper ways to make it unique enough for your tasks. For example a safe pick is to seed RC4 with /dev/urandom, and generate a pseudo random stream from that. A simpler solution is to use a combination of unix time with microseconds resolution, concatenating it with a client ID, it is not as safe, but probably up to the task in most environments."
msgstr ""

#: ../../source/topics/distlock.rst:137
# 6d719cf7db914301afc90619d523948e
msgid "The time we use as the key time to live, is called the “lock validity time”. It is both the auto release time, and the time the client has in order to perform the operation required before another client may be able to acquire the lock again, without technically violating the mutual exclusion guarantee, which is only limited to a given window of time from the moment the lock is acquired."
msgstr ""

#: ../../source/topics/distlock.rst:144
# 22407b6906404e339803caee228b143f
msgid "So now we have a good way to acquire and release the lock. The system, reasoning about a non-distrubited system which is composed of a single instance, always available, is safe. Let’s extend the concept to a distributed system where we don’t have such guarantees."
msgstr ""

#: ../../source/topics/distlock.rst:150
# e4ac4c3d9ad34f23901be2e8bd2af75a
msgid "The Redlock algorithm"
msgstr ""

#: ../../source/topics/distlock.rst:152
# 50d339d215a04d8db4e7b5c8f404df48
msgid "In the distributed version of the algorithm we assume to have N Redis masters. Those nodes are totally independent, so we don’t use replication or any other implicit coordination system. We already described how to acquire and release the lock safely in a single instance. We give for granted that the algorithm will use this method to acquire and release the lock in a single instance. In our examples we set N=5, which is a reasonable value, so we need to run 5 Redis masters in different computers or virtual machines in order to ensure that they’ll fail in a mostly independent way."
msgstr ""

#: ../../source/topics/distlock.rst:162
# 1211517c416a4b9fa27bec52173321e4
msgid "In order to acquire the lock, the client performs the following operations:"
msgstr ""

#: ../../source/topics/distlock.rst:165
# f429f649032f477fb0ca2e5ce68e21ca
msgid "It gets the current time in milliseconds."
msgstr ""

#: ../../source/topics/distlock.rst:166
# 1c20d4f98bc84877a8c3531641c1d7a1
msgid "It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During the step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client to remain blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP."
msgstr ""

#: ../../source/topics/distlock.rst:175
# 78e7c9051556479198c2b666f698d596
msgid "The client computes how much time elapsed in order to acquire the lock, by subtracting to the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired."
msgstr ""

#: ../../source/topics/distlock.rst:181
# f5176dba4c1043499c9c8f49ada58a58
msgid "If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3."
msgstr ""

#: ../../source/topics/distlock.rst:183
# 6183a34013574ff986eb767caef82003
msgid "If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believe it was not able to lock)."
msgstr ""

#: ../../source/topics/distlock.rst:189
# 6b8ffffbfcc544efb59fa7db08f6603a
msgid "Is the algorithm asynchronous?"
msgstr ""

#: ../../source/topics/distlock.rst:191
# 2cd031826c7b40de9d872a32e067204c
msgid "The algorithm relies on the assumption that while there is no synchronized clock across the processes, still the local time in every process flows approximately at the same rate, with an error which is small compared to the auto-release time of the lock. This assumption closely resembles a real-world computer: every computer has a local clock and we can usually rely on different computers to have a clock drift which is small."
msgstr ""

#: ../../source/topics/distlock.rst:199
# 31390f7e471b4d56b476d90c60659d22
msgid "At this point we need to better specifiy our mutual exclusion rule: it is guaranteed only as long as the client holding the lock will terminate its work within the lock validity time (as obtained in step 3), minus some time (just a few milliseconds in order to compensate for clock drift between processes)."
msgstr ""

#: ../../source/topics/distlock.rst:205
# 1fc034721e9b43ed8e7c7db7365b4867
msgid "For more information about similar systems requiring a bound *clock drift*, this paper is an interesting reference: `Leases: an efficient fault-tolerant mechanism for distributed file cache consistency <http://dl.acm.org/citation.cfm?id=74870>`__."
msgstr ""

#: ../../source/topics/distlock.rst:211
# 089c5e8caa8745b2b1088b1f4ca11450
msgid "Retry on failure"
msgstr ""

#: ../../source/topics/distlock.rst:213
# aa85b867086c4b55a740d0d6cfc4cfb2
msgid "When a client is not able to acquire the lock, it should try again after a random delay in order to try to desynchronize multiple clients trying to acquire the lock, for the same resource, at the same time (this may result in a split brain condition where nobody wins). Also the faster a client will try to acquire the lock in the majority of Redis instances, the less window for a split brain condition (and the need for a retry), so ideally the client should try to send the SET commands to the N instances at the same time using multiplexing."
msgstr ""

#: ../../source/topics/distlock.rst:222
# 987777a019cc4b5b93146cf5bc410791
msgid "It is worth to stress how important is for the clients that failed to acquire the majority of locks, to release the (partially) acquired locks ASAP, so that there is no need to wait for keys expiry in order for the lock to be acquired again (however if a network partition happens and the client is no longer able to communicate with the Redis instances, there is to pay an availability penalty and wait for the expires)."
msgstr ""

#: ../../source/topics/distlock.rst:230
# a63d9339eaf64b5d84d81098e57350b8
msgid "Releasing the lock"
msgstr ""

#: ../../source/topics/distlock.rst:232
# 9909a7c00bd14000b14e3e7b8dc6d71e
msgid "Releasing the lock is simple and involves just to release the lock in all the instances, regardless of the fact the client believe it was able to successfully lock a given instance."
msgstr ""

#: ../../source/topics/distlock.rst:237
# ed0ce38b7c8c4f6886e64e7d9b579156
msgid "Safety arguments"
msgstr ""

#: ../../source/topics/distlock.rst:239
# 4afa85eab5d2482e811f73397d547fbd
msgid "Is the algorithm safe? We can try to understand what happens in different scenarios."
msgstr ""

#: ../../source/topics/distlock.rst:242
# 8c9ca9cf671b41c6a7b7160e10665678
msgid "To start let’s assume that a client is able to acquire the lock in the majority of instances. All the instances will contain a key with the same time to live. However the key was set at different times, so the keys will also expire at different times. However if the first key was set at worst at time T1 (the time we sample before contacting the first server) and the last key was set at worst at time T2 (the time we obtained the reply from the last server), we are sure that the first key to expire in the set will exist for at least ``MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT``. All the other keys will expire later, so we are sure that the keys will be simultaneously set for at least this time."
msgstr ""

#: ../../source/topics/distlock.rst:254
# c34f1dd4c4a14f2f99fe4db078c1ae9c
msgid "During the time the majority of keys are set, another client will not be able to acquire the lock, since N/2+1 SET NX operations can’t succeed if N/2+1 keys already exist. So if a lock was acquired, it is not possible to re-acquire it at the same time (violating the mutual exclusion property)."
msgstr ""

#: ../../source/topics/distlock.rst:260
# e2693f5969e044ccbf10b40dc3be3d78
msgid "However we want to also make sure that multiple clients trying to acquire the lock at the same time can’t simultaneously succeed."
msgstr ""

#: ../../source/topics/distlock.rst:263
# ec853d676b83473daa7158668fc3d7ba
msgid "If a client locked the majority of instances using a time near, or greater, than the lock maximum validity time (the TTL we use for SET basically), it will consider the lock invalid and will unlock the instances, so we only need to consider the case where a client was able to lock the majority of instances in a time which is less than the validity time. In this case for the argument already expressed above, for ``MIN_VALIDITY`` no client should be able to re-acquire the lock. So multiple clients will be albe to lock N/2+1 instances at the same time (with “time\" being the end of Step 2) only when the time to lock the majority was greater than the TTL time, making the lock invalid."
msgstr ""

#: ../../source/topics/distlock.rst:274
# 4eafbe51f915448095a6c9af6fbe3f23
msgid "Are you able to provide a formal proof of safety, point out to existing algorithms that are similar enough, or to find a bug? That would be very appreciated."
msgstr ""

#: ../../source/topics/distlock.rst:279
# ebc39c33d05e4470b1937ed64b5e8b56
msgid "Liveness arguments"
msgstr ""

#: ../../source/topics/distlock.rst:281
# 01849f4d11f44f33ad39307ac90f4e27
msgid "The system liveness is based on three main features:"
msgstr ""

#: ../../source/topics/distlock.rst:283
# 4cb04a6a1cb74a969f5a4277d1ce283e
msgid "The auto release of the lock (since keys expire): eventually keys are available again to be locked."
msgstr ""

#: ../../source/topics/distlock.rst:285
# fd761cd62f3b472190a95e5e7ce296d6
msgid "The fact that clients, usually, will cooperate removing the locks when the lock was not acquired, or when the lock was acquired and the work terminated, making it likely that we don’t have to wait for keys to expire to re-acquire the lock."
msgstr ""

#: ../../source/topics/distlock.rst:289
# 6e57c8c45bfc40e7ac354d0bb02ead34
msgid "The fact that when a client needs to retry a lock, it waits a time which is comparable greater to the time needed to acquire the majority of locks, in order to probabilistically make split brain conditions during resource contention unlikely."
msgstr ""

#: ../../source/topics/distlock.rst:294
# ac2bd604d6984d57ae2248cb5b54d107
msgid "However we pay an availability penalty equal to “TTL” time on network partitions, so if there are continuous partitions, we can pay this penalty indefinitely. This happens every time a client acquires a lock and gets partitioned away before being able to remove the lock."
msgstr ""

#: ../../source/topics/distlock.rst:299
# 0056be929e384e1f9ce60b879d3e47f6
msgid "Basically if there are infinite continuous network partitions, the system may become not available for an infinite amount of time."
msgstr ""

#: ../../source/topics/distlock.rst:303
# 10fb78f0ee234393af1cad3b19d72846
msgid "Performance, crash-recovery and fsync"
msgstr ""

#: ../../source/topics/distlock.rst:305
# 20535a9ec09e450dbfcafd3dcd0516cd
msgid "Many users using Redis as a lock server need high performance in terms of both latency to acquire and release a lock, and number of acquire / release operations that it is possible to perform per second. In order to meet this requirement, the strategy to talk with the N Redis servers to reduce latency is definitely multiplexing (or poor’s man multiplexing, which is, putting the socket in non-blocking mode, send all the commands, and read all the commands later, assuming that the RTT between the client and each instance is similar)."
msgstr ""

#: ../../source/topics/distlock.rst:314
# a9dabe9a1178487c81a1d99d8c09a65f
msgid "However there is another consideration to do about persistence if we want to target a crash-recovery system model."
msgstr ""

#: ../../source/topics/distlock.rst:317
# 798b38bb3f594e788018030ebe74d624
msgid "Basically to see the problem here, let’s assume we configure Redis without persistence at all. A client acquires the lock in 3 of 5 instances. One of the instances where the client was able to acquire the lock is restarted, at this point there are again 3 instances that we can lock for the same resource, and another client can lock it again, violating the safety property of exclusivity of lock."
msgstr ""

#: ../../source/topics/distlock.rst:324
# 82c49d0806524a77b21e5f8078d40f5d
msgid "If we enable AOF persistence, things will improve quite a bit. For example we can upgrade a server by sending SHUTDOWN and restarting it. Because Redis expires are semantically implemented so that virtually the time still elapses when the server is off, all our requirements are fine. However everything is fine as long as it is a clean shutdown. What about a power outage? If Redis is configured, as by default, to fsync on disk every second, it is possible that after a restart our key is missing. In theory, if we want to guarantee the lock safety in the face of any kind of instance restart, we need to enable fsync=always in the persistence setting. This in turn will totally ruin performances to the same level of CP systems that are traditionally used to implement distributed locks in a safe way."
msgstr ""

#: ../../source/topics/distlock.rst:337
# de5bccc7c78646a4a45187f7a672ee92
msgid "However things are better than what they look like at a first glance. Basically the algorithm safety is retained as long as when an instance restarts after a crash, it no longer participates to any **currently active** lock, so that the set of currently active locks when the instance restarts, were all obtained by locking instances other than the one which is rejoining the system."
msgstr ""

#: ../../source/topics/distlock.rst:344
# be311a02cd2344cea926442173426d67
msgid "To guarantee this we just need to make an instance, after a crash, unavailable for at least a bit more than the max ``TTL`` we use, which is, the time needed for all the keys about the locks that existed when the instance crashed, to become invalid and be automatically released."
msgstr ""

#: ../../source/topics/distlock.rst:349
# d301cf02eb564096b30f4036021ff2fb
msgid "Using *delayed restarts* it is basically possible to achieve safety even without any kind of Redis persistence available, however note that this may translate into an availability penalty. For example if a majority of instances crash, the system will become gobally unavailable for ``TTL`` (here globally means that no resource at all will be lockable during this time)."
msgstr ""

#: ../../source/topics/distlock.rst:357
# 8c7bb651d842447a93831ce13e884baf
msgid "Making the algorithm more reliable: Extending the lock"
msgstr ""

#: ../../source/topics/distlock.rst:359
# aaeef6fd0f174a68ae87b3071601d18c
msgid "If the work performed by clients is composed of small steps, it is possible to use smaller lock validity times by default, and extend the algorithm implementing a lock extension mechanism. Basically the client, if in the middle of the computation while the lock validity is approaching a low value, may extend the lock by sending a Lua script to all the instances that extends the TTL of the key if the key exists and its value is still the random value the client assigned when the lock was acquired."
msgstr ""

#: ../../source/topics/distlock.rst:368
# 51d00f195cb247d89c0135bec77fefc4
msgid "The client should only consider the lock re-acquired if it was albe to extend the lock into the majority of instances, and within the validity time (basically the algorithm to use is very similar to the one used when acquiring the lock)."
msgstr ""

#: ../../source/topics/distlock.rst:373
# 1bf8fe6f9c5044b392b11a9d312a0be1
msgid "However this does not technically change the algorithm, so anyway the max number of locks reacquiring attempts should be limited, otherwise one of the liveness properties is violated."
msgstr ""

#: ../../source/topics/distlock.rst:378
# 33936d747c114b33af1c9f8cb725e6db
msgid "Want to help?"
msgstr ""

#: ../../source/topics/distlock.rst:380
# a63afdb7339f43a88470f653d1d8dff5
msgid "If you are into distributed systems, it would be great to have your opinion / analysis. Also reference implementations in other languages could be great."
msgstr ""

#: ../../source/topics/distlock.rst:384
# 04b8b42bedb54495bf7fd5d827b8f4c3
msgid "Thanks in advance!"
msgstr ""

